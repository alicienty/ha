{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alicienty/ha/blob/main/Alice_Lukyanchikova_Task1_advanced_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 1: разработка пайплайна предобработки текста с ООП\n",
        "\n",
        "#### Цель:\n",
        "Hазработать класс на Python с использованием принципов объектно-ориентированного программирования (ООП), который реализует пайплайн для предобработки текста\n",
        "\n",
        "#### Методы, которые должен реализовывать разработанный класс:\n",
        "1. Токенизация\n",
        "2. Лемматизация\n",
        "3. Удаление стоп-слов\n",
        "\n",
        "Инструкция содержит подробное описание процесса создания класса. Результат вашей работы разместите в одной ячейке ниже инструкции"
      ],
      "metadata": {
        "id": "LHe6Z-d9h1Ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Инструкция\n",
        "\n",
        "**Создание класса**\n",
        "\n",
        "Определите класс `TextProcessor`, который будет содержать методы для предобработки текста.  \n",
        "\n",
        "```python\n",
        "# Создание базового класса для предобработки текста\n",
        "class TextProcessor:\n",
        "    def __init__(self, text):\n",
        "        \"\"\"\n",
        "        Инициализация класса с исходным текстом.\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "        self.tokens = []\n",
        "        self.cleaned_tokens = []\n",
        "```\n",
        "\n",
        "**Метод токенизации**\n",
        "\n",
        "Реализуйте метод, который разделяет текст на отдельные слова.\n",
        "\n",
        "```python\n",
        "    def tokenize(self):\n",
        "        \"\"\"\n",
        "        Метод для токенизации текста.\n",
        "        \"\"\"\n",
        "        # Реализуйте токенизацию здесь\n",
        "        pass\n",
        "```\n",
        "\n",
        "Пример вызова метода:  \n",
        "```python\n",
        "processor = TextProcessor(text)\n",
        "processor.tokenize()\n",
        "print(processor.tokens)\n",
        "```\n",
        "\n",
        "**Метод лемматизации**\n",
        "\n",
        "Добавьте метод, который преобразует слова к их леммам. Используйте `WordNetLemmatizer` из NLTK.\n",
        "\n",
        "```python\n",
        "    def lemmatize(self):\n",
        "        \"\"\"\n",
        "        Метод для лемматизации токенов.\n",
        "        \"\"\"\n",
        "        # Реализуйте лемматизацию здесь\n",
        "        pass\n",
        "```\n",
        "\n",
        "Пример вызова метода:  \n",
        "```python\n",
        "processor.lemmatize()\n",
        "print(processor.cleaned_tokens)\n",
        "```\n",
        "\n",
        "**Метод удаления стоп-слов**\n",
        "\n",
        "Добавьте метод для удаления стоп-слов из токенов. Используйте список стоп-слов из NLTK.\n",
        "\n",
        "```python\n",
        "    def remove_stopwords(self):\n",
        "        \"\"\"\n",
        "        Метод для удаления стоп-слов.\n",
        "        \"\"\"\n",
        "        # Реализуйте удаление стоп-слов здесь\n",
        "        pass\n",
        "```\n",
        "\n",
        "Пример вызова метода:  \n",
        "```python\n",
        "processor.remove_stopwords()\n",
        "print(processor.cleaned_tokens)\n",
        "```\n",
        "\n",
        "**Запуск пайплайна**\n",
        "\n",
        "Объедините все шаги в пайплайн. Добавьте вызов каждого метода по порядку:\n",
        "\n",
        "```python\n",
        "processor = TextProcessor(text)\n",
        "processor.tokenize()\n",
        "processor.remove_stopwords()\n",
        "processor.lemmatize()\n",
        "\n",
        "# Вывод итогового результата\n",
        "print(\"Токены:\", processor.tokens)\n",
        "print(\"Лемматизированные токены:\", processor.cleaned_tokens)\n",
        "```"
      ],
      "metadata": {
        "id": "Z4PuMmgPimOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### ВАШЕ РЕШЕНИЕ ЗДЕСЬ\n",
        "\n",
        "import nltk #импортируем все необходимые библиотеки и модули\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "### Сначала опишите класс\n",
        "class TextProcessor:\n",
        "\n",
        "    def __init__(self, text):\n",
        "      \"\"\"\n",
        "      Инициализация класса с исходным текстом.\n",
        "      \"\"\"\n",
        "      self.text = text #переменная с текстом\n",
        "      self.tokens = [] #переменная с токенами\n",
        "      self.cleaned_tokens = [] #переменная с чистым текстом\n",
        "      #self.lemmatized_text = []\n",
        "\n",
        "    def tokenize(self): #определяем функцию для токенизации\n",
        "      \"\"\"\n",
        "      Метод для токенизации текста.\n",
        "      \"\"\"\n",
        "      self.tokens = word_tokenize(self.text)\n",
        "      self.tokens = [word for word in self.tokens if re.match(r'^[a-zA-Z]+$', word)]\n",
        "      #TODO: чистка пунктуации, лишних символов\n",
        "\n",
        "      return self.tokens\n",
        "\n",
        "    def lemmatize(self):\n",
        "      \"\"\"\n",
        "      Метод для лемматизации токенов.\n",
        "      \"\"\"\n",
        "      self.tokenize() #запускаем функцию токенизации, чтобы лемматизатор...\n",
        "      #...получал корректные данные на вход (список слов), иначе функция, если вызывать ее отдельно, вернет пустой список\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      self.cleaned_tokens = [lemmatizer.lemmatize(word.lower()) for word in self.tokens]\n",
        "      return self.cleaned_tokens\n",
        "\n",
        "    def remove_stopwords(self):\n",
        "\n",
        "      stopwords_1 = stopwords.words(\"english\")\n",
        "      self.cleaned_tokens = [word for word in self.cleaned_tokens if word not in stopwords_1]\n",
        "      return self.cleaned_tokens\n",
        "\n",
        "    def process(self):\n",
        "      self.tokenize()\n",
        "      self.lemmatize()\n",
        "      self.remove_stopwords()\n",
        "\n",
        "### В процессе реализации класса, вызывайте каждый метод,\n",
        "### чтобы убедиться в том, что программа выполняется корректно\n",
        "\n",
        "### В конце работы запустите пайплайн:\n",
        "### вызовите все методы согласно инструкции\n",
        "text = \"There was nothing else to do, so Alice soon began talking again.\"\n",
        "processor = TextProcessor(text)\n",
        "processor.tokenize()\n",
        "processor.lemmatize()\n",
        "processor.remove_stopwords()\n",
        "processor.process()\n",
        "\n",
        "print(\"Предобработанные токены:\", processor.cleaned_tokens)\n",
        "print(\"Исходные токены:\", processor.tokens)"
      ],
      "metadata": {
        "id": "UHqMdoUip-We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e8557d3-a887-458b-c0eb-a00a950e7b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предобработанный текст: ['wa', 'nothing', 'else', 'alice', 'soon', 'began', 'talking']\n",
            "Текст: ['There', 'was', 'nothing', 'else', 'to', 'do', 'so', 'Alice', 'soon', 'began', 'talking', 'again']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "P.S Ниже черновиковые ячейки промежуточной работы, решила не удалять"
      ],
      "metadata": {
        "id": "CkI1vIsxYfxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"There was nothing else to do, so Alice soon began talking again.\""
      ],
      "metadata": {
        "id": "baoBxWniTHiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "x = []\n",
        "for i in tokens_ex:\n",
        "  i_cl = i.lower()\n",
        "  i_cl = wnl.lemmatize(i_cl)\n",
        "  x.append(i_cl)"
      ],
      "metadata": {
        "id": "TbBSO9X8jXuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3enSSxZShLQx",
        "outputId": "020b92c6-93ec-4390-f6bb-0726b47e9986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['there',\n",
              " 'wa',\n",
              " 'nothing',\n",
              " 'else',\n",
              " 'to',\n",
              " 'do',\n",
              " ',',\n",
              " 'so',\n",
              " 'alice',\n",
              " 'soon',\n",
              " 'began',\n",
              " 'talking',\n",
              " 'again',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = TextProcessor(text)\n",
        "processor.lemmatize()\n",
        "print(processor.cleaned_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5aWYj72T1PX",
        "outputId": "a50423d4-8875-4422-a04b-b907a8cb1e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['there', 'wa', 'nothing', 'else', 'to', 'do', ',', 'so', 'alice', 'soon', 'began', 'talking', 'again', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = TextProcessor(text)\n",
        "processor.remove_stopwords()\n",
        "print(processor.cleaned_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uttzUGe-HLUw",
        "outputId": "36ade813-422f-480a-cdce-8517b053e6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wa', 'nothing', 'else', 'do', ',', 'alice', 'soon', 'began', 'talking', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor.cleaned_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCatJmVJ5lYE",
        "outputId": "6dbdabb2-be5c-4f36-b1d3-6dbb53862134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['there',\n",
              " 'wa',\n",
              " 'nothing',\n",
              " 'else',\n",
              " 'to',\n",
              " 'do',\n",
              " ',',\n",
              " 'so',\n",
              " 'alice',\n",
              " 'soon',\n",
              " 'began',\n",
              " 'talking',\n",
              " 'again',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"There was nothing else to do, so Alice soon began talking again.\"\n",
        "processor = TextProcessor(text)\n",
        "processor.process()\n",
        "\n",
        "print(\"preprocessed:\", processor.cleaned_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhtvZ6SzTIsn",
        "outputId": "84753118-bdf6-4d5f-80bb-32a533a99daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessed: ['wa', 'nothing', 'else', 'do', ',', 'alice', 'soon', 'began', 'talking', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Критерии оценивания**\n",
        "\n",
        "- **Отсутствие ошибок в ООП:** класс корректно инициализируется и выполняет все методы без ошибок (2 балла).  \n",
        "- **Реализован метод токенизации:** текст корректно разделяется на токены (2 балла).  \n",
        "- **Реализован метод лемматизации:** все токены преобразованы к леммам (2 балла).  \n",
        "- **Реализован метод удаления стоп-слов:** стоп-слова корректно удалены из токенов (2 балла).  \n",
        "- **Класс протестирован:** все методы вызваны, код работает (2 балла).  \n",
        "\n",
        "Общий балл: **10 баллов**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Примечания**\n",
        "\n",
        "- Добавьте текстовые ячейки в Colab и комментарии с описанием этапов выполнения.\n",
        "- Комментарии не оцениваются, но они важны для вашей работы и воспроизводимости кода\n",
        "- Проверьте, что все методы выполняются корректно на примере любого текста.\n",
        "- Пример текста для проверки работы пайплайна: `https://github.com/vifirsanova/compling/blob/main/tasks/task1/data.txt`."
      ],
      "metadata": {
        "id": "Kk81rQiAqBjr"
      }
    }
  ]
}